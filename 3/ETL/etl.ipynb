{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conectado a MongoDB en mongodb://localhost:27017/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "import psycopg2\n",
    "from sklearn.decomposition import PCA\n",
    "from pymongo.errors import PyMongoError\n",
    "\n",
    "# Establecer la conexión a MongoDB\n",
    "primary_uri = 'mongodb://localhost:27017/'\n",
    "secondary_uri = 'mongodb://localhost:27018/'\n",
    "try:\n",
    "    mongo_client = MongoClient(primary_uri, serverSelectionTimeoutMS=2000)\n",
    "    mongo_client.server_info()  # Prueba de conexión\n",
    "    print(f\"Conectado a MongoDB en {primary_uri}\")\n",
    "except PyMongoError:\n",
    "    print(f\"No se pudo conectar a MongoDB en {primary_uri}\")\n",
    "\n",
    "    # Si la conexión falla, intenta la instancia secundaria\n",
    "    try:\n",
    "        mongo_client = MongoClient(secondary_uri, serverSelectionTimeoutMS=2000)\n",
    "        mongo_client.server_info()  # Prueba de conexión\n",
    "        print(f\"Conectado a MongoDB en {secondary_uri}\")\n",
    "    except PyMongoError:\n",
    "        print(f\"No se pudo conectar a MongoDB en {secondary_uri}\")\n",
    "        mongo_client = None\n",
    "\n",
    "\n",
    "mongo_db = mongo_client['MyDatabase']\n",
    "mongo_collection = mongo_db['MyCollection']\n",
    "\n",
    "# Obtener los datos de la colección de MongoDB con proyección y límite\n",
    "projection = {'_id': 0, 'value': 1, 'wm_yr_wk': 1, 'wday': 1, 'month': 1, 'year': 1, 'snap_CA': 1, 'snap_TX': 1,\n",
    "              'sell_price': 1, 'dept_id': 1, 'week': 1, 'day': 1}\n",
    "limit = 1000  # Número de documentos a obtener\n",
    "data = list(mongo_collection.find({}, projection=projection).limit(limit))\n",
    "\n",
    "# Crear un DataFrame de Pandas a partir de los datos\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "##MINERIA DE DATOS\n",
    "# Eliminar datos vacíos\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Eliminar datos duplicados\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Cambiar nombres de columnas\n",
    "df.rename(columns={'day': 'dia', 'month': 'mes', 'year': 'anio'}, inplace=True)\n",
    "\n",
    "# Exportar el nuevo DataFrame a un archivo CSV\n",
    "csv_file_path = 'Dataframe.csv'\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "# Aplicar PCA para reducir la dimensionalidad\n",
    "pca = PCA(n_components=3)\n",
    "df_pca = pca.fit_transform(df.iloc[:, 1:])  # Excluye la columna \"value\" en la transformación PCA\n",
    "\n",
    "# Agregar las componentes principales al DataFrame\n",
    "df['pca_component_1'] = df_pca[:, 0]  \n",
    "df['pca_component_2'] = df_pca[:, 1]  \n",
    "df['pca_component_3'] = df_pca[:, 2]  \n",
    "\n",
    "# Eliminar columnas no importantes o redundantes\n",
    "df.drop(['value'], axis=1, inplace=True)  # Elimina la columna \"value\" original\n",
    "\n",
    "# Exportar el nuevo DataFrame a otros contenedores de Docker de MongoDB\n",
    "nuevoMongoCliente1 = MongoClient('mongodb://localhost:27050/')\n",
    "nuevoMongo1 = nuevoMongoCliente1['DataProcesada']\n",
    "mongoCliente1 = nuevoMongo1['Dataframe']\n",
    "\n",
    "nuevoMongoCliente2 = MongoClient('mongodb://localhost:27051/')\n",
    "nuevoMongo2 = nuevoMongoCliente2['DataProcesada']\n",
    "mongoCliente2 = nuevoMongo2['Dataframe']\n",
    "\n",
    "nuevoMongoCliente3 = MongoClient('mongodb://localhost:27052/')\n",
    "nuevoMongo3 = nuevoMongoCliente3['DataProcesada']\n",
    "mongoCliente3 = nuevoMongo3['Dataframe']\n",
    "\n",
    "# Convertir el DataFrame a una lista\n",
    "df_dict_list = df.to_dict(orient='records')\n",
    "\n",
    "# Insertar los datos en los otros contenedores\n",
    "mongoCliente1.insert_many(df_dict_list)\n",
    "mongoCliente2.insert_many(df_dict_list)\n",
    "mongoCliente3.insert_many(df_dict_list)\n",
    "\n",
    "# Exportar el nuevo DataFrame a tres bases de datos PostgreSQL en lotes\n",
    "pg_connection1 = psycopg2.connect(\n",
    "    host='localhost',\n",
    "    port=5440,\n",
    "    user='postgres',\n",
    "    password='admin',\n",
    "    database='data'\n",
    ")\n",
    "\n",
    "pg_connection2 = psycopg2.connect(\n",
    "    host='localhost',\n",
    "    port=5441,\n",
    "    user='postgres',\n",
    "    password='admin',\n",
    "    database='data'\n",
    ")\n",
    "\n",
    "pg_connection3 = psycopg2.connect(\n",
    "    host='localhost',\n",
    "    port=5442,\n",
    "    user='postgres',\n",
    "    password='admin',\n",
    "    database='data'\n",
    ")\n",
    "\n",
    "pg_cursor1 = pg_connection1.cursor()\n",
    "pg_cursor2 = pg_connection2.cursor()\n",
    "pg_cursor3 = pg_connection3.cursor()\n",
    "\n",
    "# Crear una tabla en PostgreSQL para almacenar los datos preprocesados en cada base de datos\n",
    "pg_create_table_query = \"CREATE TABLE IF NOT EXISTS Dataframe (wm_yr_wk INT, wday INT, mes INT, anio INT, snap_CA INT, snap_TX INT, sell_price FLOAT, dept_id INT, week INT, dia INT, pca_component_1 FLOAT, pca_component_2 FLOAT, pca_component_3 FLOAT)\"\n",
    "pg_cursor1.execute(pg_create_table_query)\n",
    "pg_cursor2.execute(pg_create_table_query)\n",
    "pg_cursor3.execute(pg_create_table_query)\n",
    "\n",
    "# Generar los valores para la consulta INSERT INTO\n",
    "values = ', '.join(['%s'] * len(df.columns))\n",
    "pg_insert_query = f\"INSERT INTO Dataframe ({', '.join(df.columns)}) VALUES ({values})\"\n",
    "\n",
    "# Convertir los datos preprocesados a una lista de tuplas\n",
    "valoresLote = [tuple(row) for row in df.values]\n",
    "\n",
    "# Insertar los datos en lotes utilizando executemany en cada base de datos\n",
    "tamañoLote = 1000\n",
    "for i in range(0, len(valoresLote), tamañoLote):\n",
    "    dataLote = valoresLote[i:i+tamañoLote]\n",
    "    pg_cursor1.executemany(pg_insert_query, dataLote)\n",
    "    pg_cursor2.executemany(pg_insert_query, dataLote)\n",
    "    pg_cursor3.executemany(pg_insert_query, dataLote)\n",
    "\n",
    "# Confirmar los cambios en las bases de datos PostgreSQL\n",
    "pg_connection1.commit()\n",
    "pg_connection2.commit()\n",
    "pg_connection3.commit()\n",
    "\n",
    "# Cerrar la conexión a las bases de datos PostgreSQL\n",
    "pg_cursor1.close()\n",
    "pg_cursor2.close()\n",
    "pg_cursor3.close()\n",
    "pg_connection1.close()\n",
    "pg_connection2.close()\n",
    "pg_connection3.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in c:\\users\\dajar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in c:\\users\\dajar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pymongo) (2.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pymongo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
